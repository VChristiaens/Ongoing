{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistical Inference: Part 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "\n",
    "- I. [Basic probability concepts](#I)    \n",
    "    * I.1 [Probability Axioms](Basic-statistics_01_Expanded.ipynb)\n",
    "    * I.2 [What is a random variable ?](#I.2-What-is-a-random-variable-?)\n",
    "    * I.3 [What is a conditional probability $p(A|B)p(A | B)$ and Bayes theorem ?](#I.3-What-is-a-conditional-probability-$p(A-|-B)$-and-Bayes-theorem?)\n",
    "    * I.4 [What is a probability density function ?](Basic-statistics_01_Part1b.ipynb/#pdf)\n",
    "    * I.4b [Bivariate PDF](Basic-statistics_01_Part1b.ipynb/#bivariate)\n",
    "    * I.5 [What is a cumulative distribution function ?](Basic-statistics_01_Part1b.ipynb/#I.5-What-is-a-cumulative-distribution-function-?)\n",
    "    * I.6 [Working with pdf and CDF](Basic-statistics_02.ipynb)\n",
    "\n",
    "\n",
    "- [Descriptive statistics](#Intermezo:-Descriptive-statistics)\n",
    "    * To review the **properties of probability distribution functions** (mean, standard deviation, variance, skewness, ...), go to the notebook [Descriptive_statistics_01.ipynb](Descriptive_statistics_01.ipynb).    \n",
    "    * To **better understand visually the link between PDF and CDF**, and do an interactive tour of the characteristics and properties of **common distributions** encountered in science, then, run the Notebook [Descriptive_statistics_02.ipynb](Descriptive_statistics_02.ipynb). \n",
    "\n",
    "- X. [References and supplementary material](#X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to be used in this notebook\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Why some statistics ? \n",
    "\n",
    "With this lecture, we aim to give you tools to manipulate data (from observations or from simulations). Data, even when being the outcome of some deterministic process, contain a stochastic component, for example due to the observational device. This means that in essence, data are what statisticians call a **random variable**. This is why, when you report measurements you also need to report some uncertainties. These uncertainty calculation look often to student like some kind of black magic. This also means that they are sometimes not adequately used or reported. We'll clarify in the coming lectures, how to make those calculations using python tools. \n",
    "\n",
    "This is a necessary step to enter into the field of machine learning. If you look on Wikipedia at the page dedicated to [Machine learning](https://en.wikipedia.org/wiki/Machine_learning), you'll find the following definition: \"Machine learning is the study of computer algorithms that improve automatically through experience. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.\" This is not the result of black magic, but of well understood statistic and probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Basic (probability) concepts:  <a class=\"anchor\" id=\"I\"></a>\n",
    "\n",
    "This section aims at reminding us some definitions and notations, so everybody is on the same page. Illustration with python will also be provided. \n",
    "\n",
    "In probability theory, we define the following:\n",
    "- the set of all possible outcomes of an experiment is generally called **sample space** and denoted, $\\Omega$. \n",
    "- Points in $\\Omega$, are called **realisations** $\\omega$. In other words each of your *data point* is a realisation (but there may exist other  realisations that are not in your data). \n",
    "- An **Event** is a subset of $\\Omega$ (i.e. a subsample of the $\\omega$). In other words your *data set* is an event.  \n",
    "- $p(A)$ is the **probability** of an event A. ($p(A)$ can also refer to a probability that a value of $x$ falls in a d$x$ wide interval around $x$. )\n",
    "\n",
    "> The *frequentist (classical) interpretation of statistics*: You make a measurement, you repeat it an infinite number of times, and build a histogram. This histogram gives the relative frequency of getting a measurement compared to another. This relative frequency is what we call a probability. The whole possible outcomes of an experiment is called sample space $\\Omega$. An event is a subsample of $\\omega$ / is an ensemble of realisations (your data set).\n",
    "\n",
    "**Example**: We measure the magnitude $m$ of a variable star. Then $\\Omega = {\\rm I\\!R}$ (or some plausible range such as $]-30, 40 [ $, but this makes little difference in practice and $\\Omega$ can in general be larger than needed). $\\omega$ is a value in $\\Omega$. While an event is the ensemble of magnitude measurements  that are e.g. such that $m_i > 15$ and $m_i <=15.5$, hence this *event* is A = ]15, 15.5]. Note that an event *can* also be a single point (i.e. A = [17.]). \n",
    "\n",
    "If you need some refresh on basic probability concepts, move to the [expanded version](Basic-statistics_01_Expanded.ipynb) of this Notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 Probability axioms\n",
    "\n",
    "Check the [expanded version](Basic-statistics_01_Expanded.ipynb) of this Notebook to refresh your mind about the axioms behind the probability theory. \n",
    "\n",
    "### I.2 What is a random variable ?\n",
    "\n",
    "Definition (informal): \n",
    "\n",
    "> A random variable is a variable whose value results from the measurement of a quantity that is subject to random variations. \n",
    "\n",
    "**Example:** You roll a dice. You can then define $X(\\omega)$ be the *number of times you draw an even number* in a sequence $\\omega$. Imagine you have rolled your dice 10 times, and get  $\\omega = {1, {\\bf 2, 4, 2}, 5, {\\bf 4, 6}, 3, 3, 1 }$. Then $X(\\omega) = 5$ is the value taken by your *random variable* X. \n",
    "\n",
    "Note that you can have either *continuous* or *discrete* random variables. If they form a countable set, they are discrete. They are continuous otherwise. \n",
    "\n",
    "**Warning**, a random variable is *NOT* like typical mathematical variables as it can take *different values* (each with an associated probability). In litterature, you will generaly find random variables written with *CAPITAL* letters, and their particular *realization* with *lowercase*. A realisation of a random variable is also called \"random variate\". \n",
    "\n",
    "**Random variable with numpy**\n",
    "\n",
    "In numpy there is a sub-module called random that contains many functions that involve random selection. In particular there is a function `choice` that allows you to make a choice at random. It picks one item at random from an array, and it is equally likely to pick any of the items. The function call is `np.random.choice(array_name)`, where `array_name` is the name of the array from which to make the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'galaxy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a small array with classification of objects on a CCD: \n",
    "class_phot = np.array(['star', 'galaxy', 'galaxy', 'star', 'star'])\n",
    "np.random.choice(class_phot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you re-rerun the above cell multiple times, you will see that the output changes, because the output is taken at random. If you provide a second argument (`size = n`), it will repeat the process $n$ number of times. In practice, this `method` allows you to *generate a random sample* from a 1-D array. You can add a third argument that specifies if you allow replacements or not as the output: i.e. `replace=False` means that the value stored at a given index of the array will only appear once in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(class_phot, size=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use e.g. np.arange(5) and size=5 to compare the behaviour with replace=True / replace = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try out with an array size > size of the random sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If size > size(a), you cannot generate a random sample with choice and replace=False (it does not make sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important methods of `np.random` are:\n",
    "- `np.random.seed(value)`: Set the seed of the random number generator (allows to get reproductible results)\n",
    "- `np.random.RandomState(seed = value)`: Better way to fix the seed -in particular in case of parallelization: see https://stackoverflow.com/questions/16016959/scipy-stats-seed) \n",
    "- `np.random.rand(shape)`: generate an array populated with random floats drawn from an uniform distribution. \n",
    "- `np.random.randint(low, high, shape)`: generate an array of random integers between `low` and `high` values. \n",
    "- `np.random.permutation(array_name)`: return a copy of the original array with elements permuted along the first axis of the array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Coin toss\n",
    "\n",
    "Create a 2D array of 10 rows and 7 columns (with random integer values). \n",
    "- (1) Use `choice` to select at random 5 different rows. \n",
    "- (2) Use `choice` to select at random 3 different columns.\n",
    "- (3) Create an 1-D array representing result of 1000 coin tosses (set value `0` for head and `1` for tail using random integers generator). Plot the histogram of their values (using `matplotlib.pyplot.hist`; Note that you can set the number of bins $N$ by setting the argument `bins = N`). Set the argument `density` of `hist()` to values True and False. What is the difference of behaviour ? \n",
    "- (4) Fix the seed of your random generator. Create an array `a` of 10 integers between 0 and 10. Redo this operation and save it into an array `b`. Compare `a` and `b`. How should you proceed to have 2 identical arrays ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select at random 5 different rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3 columns ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 1D array representing 1000 coin toss and histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutations of the first row of the original array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Fix the seed of your random generator. Create an array `a` of 10 integers between 0 and 10. \n",
    "# Redo this operation and save it into an array `b`. Compare `a` and `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How should you proceed to have 2 identical arrays ?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 What is a conditional probability $p(A | B)$ and Bayes theorem? \n",
    "\n",
    "The conditional probability expresses the fact that the probability of an event can depend on another one. For example, you may wonder what is the probability to have a flu if you have fever. You would write it $p(\\rm{flu} | \\rm{fever}) $, which reads \"p of flu GIVEN fever\". $p(A \\, | \\, B) = \\frac{p(A \\, \\cap \\, B)}{p(B)}$ is the *fraction of times* $A$ occurs when $B$ occurs.\n",
    "\n",
    "The Bayes theorem expresses formally how to calculate this conditional probability:\n",
    "$$\n",
    "p(A\\,|\\,B) = \\frac{p(B\\,|\\,A) \\, p(A)}{p(B)}\n",
    "$$\n",
    "\n",
    "The main thing you have to recall with conditional probability is that $p(A | B) \\neq p( B | A)$: probability to have a flu given you have fever is different from probability you have fever given you have a flu. \n",
    "\n",
    "**Question:**  Let's consider a rare disease that affects 0.1\\% of the population. On the other hand, let's consider a test that is efficient at 99\\% (i.e. 1 \\% False positive rate). If you have a positive test, what is the probability for you to be affected by this disease ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among 1000 persons, 1 has the disease (it touches 0.1 \\% of the population). The test has 99% efficiency. Which means that 1% of the people will be tested positive while not being sick. => 10 people will be positive while healthy. You should add ~1 being positive while being effectively sick.     \n",
    "=> $p(\\rm{disease} | +) \\sim 1/11 = 9\\%$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What you are interested in is the p(disease | positive test)  \n",
    "# p(d | +) = p(+ | d) * p(d) / p(+)\n",
    "# We have p(+ | d) = 0.99 \n",
    "p_B_given_A = 0.99\n",
    "# We have probability of disease p(d) = 0.001 \n",
    "p_A = 0.001\n",
    "# We have probability of positive test = p(+ | d) * p (d) + p(+ | !d) * p(!d)\n",
    "p_B = 0.001 *0.99 + (1.-0.001) * (1 - 0.99) \n",
    "p_A_given_B = p_B_given_A * p_A / p_B\n",
    "p_A_given_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> **Be careful with RARE events !** If you build an algorithm that detects securely 99% of the rare events (e.g. a gravitational lens, a supernova, a planet around a red dwarf, ...), you may still spend a lot of time observing false positives ... As a rule of thumb, when you design an algorithm that aims at finding rare objects, **you should aim for a False Positive Rate (FPR) at least smaller or equal than the prevalence of the event**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The follow-up notebook is [Basic-statistics_01_Part1b.ipynb](Basic-statistics_01_Part1b.ipynb)**. It reviews the concepts of PDF and CDF and how to calculate them for specific distributions in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_lectures]",
   "language": "python",
   "name": "conda-env-py3_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
